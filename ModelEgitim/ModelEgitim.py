# -*- coding: utf-8 -*-
"""YazilimSinamaHumanOrAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R5ILcQr5T7od5AYqOQ8grczidiyyu0Di
"""

!pip install h2o
import pandas as pd
import h2o
from sklearn.utils import shuffle

# 1. Dosyaları yükleyelim
# Not: Dosya isimlerindeki boşluklara dikkat et (human .csv gibi)
ai_df = pd.read_csv('ai.csv')
human_df = pd.read_csv('human.csv')

# 2. Verileri birleştirelim
# Her iki dosyanın da 'Text' ve 'Label' sütunlarına sahip olduğunu varsayıyoruz
df = pd.concat([ai_df, human_df], axis=0).reset_index(drop=True)

# 3. Veriyi karıştırma (Shuffle)
# Veriler önce AI sonra Human şeklinde gelirse model öğrenirken sorun yaşayabilir
df = shuffle(df, random_state=42).reset_index(drop=True)

# İlk 5 satıra bakalım
print("Veri Seti Özeti:")
print(df.head())
print("\nSınıf Dağılımı:")
print(df['Label'].value_counts())

import re

# 1. Eksik ve Mükerrer Verileri Temizleme
print(f"Temizlik öncesi satır sayısı: {len(df)}")
df = df.dropna(subset=['Text']) # Boş metinleri sil
df = df.drop_duplicates(subset=['Text']) # Aynı metinleri sil
print(f"Temizlik sonrası satır sayısı: {len(df)}")

# 2. Metin Temizleme Fonksiyonu
def clean_text(text):
    # Küçük harfe çevir
    text = text.lower()
    # URL'leri kaldır
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    # HTML etiketlerini kaldır
    text = re.sub(r'<.*?>', '', text)
    # Rakamları ve özel karakterleri kaldır (Opsiyonel: Sayılar bazen AI tespitinde ipucu olabilir)
    # Sadece harfleri ve boşlukları tutalım:
    text = re.sub(r'[^a-z\s]', '', text)
    # Fazla boşlukları temizle
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Temizliği uygula
df['Clean_Text'] = df['Text'].apply(clean_text)

# Sonuca bir göz atalım
print("\nTemizlenmiş Metin Örneği:")
print(df[['Text', 'Clean_Text']].head(2))

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Paketleri kontrol edelim
nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

def preprocess_nlp(text):
    # Kelimelere ayır (Tokenization)
    tokens = text.split()

    # SADECE Lemmatization uygula (Stopwords temizliği kaldırıldı)
    cleaned_tokens = [
        lemmatizer.lemmatize(word)
        for word in tokens
    ]

    # Tekrar metne dönüştür
    return " ".join(cleaned_tokens)

# Ön işlemeyi yeni haliyle uygula
df['Processed_Text'] = df['Clean_Text'].apply(preprocess_nlp)

print("Stopwords Temizliği Yapılmamış Yeni Hali:")
print(df[['Clean_Text', 'Processed_Text']].head(1).values)

from sklearn.feature_extraction.text import TfidfVectorizer

# 1. TF-IDF Modelini Oluşturalım
# ngram_range=(1, 2) yaparak hem tekli kelimeleri hem de ikili kelime gruplarını (bigrams) yakalıyoruz.
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))

# 2. Metni Sayısal Matrise Dönüştürelim
tfidf_matrix = tfidf.fit_transform(df['Processed_Text'])

# 3. Matrisi H2O'nun anlayacağı bir DataFrame'e çevirelim
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())

# 4. Etiket (Label) sütununu geri ekleyelim
tfidf_df['Label'] = df['Label'].values

print(f"Yeni veri setinin boyutu: {tfidf_df.shape}")
print("İlk 5 sütun ve 5 satıra bir bakalım:")
print(tfidf_df.iloc[:5, :5])

import h2o

# 1. H2O'yu başlatalım (Bellek kapasitesine göre max_mem_size ayarlanabilir)
h2o.init()

# 2. Pandas DataFrame'i H2O Frame'ine dönüştürelim
# (Önceki adımda oluşturduğumuz tfidf_df'i kullanıyoruz)
hf = h2o.H2OFrame(tfidf_df)

# 3. Label sütununu kategorik (enum) yapalım
# Bu adım çok önemli, yoksa H2O sınıflandırma yerine regresyon yapar.
hf['Label'] = hf['Label'].asfactor()

# 4. Veriyi bölelim (Train: %70, Validation: %15, Test: %15)
train, valid, test = hf.split_frame(ratios=[0.7, 0.15], seed=42)

print(f"Eğitim seti satır sayısı: {train.nrow}")
print(f"Test seti satır sayısı: {test.nrow}")

# Hedef ve Özellik sütunlarını tanımlayalım
y = "Label"
x = hf.columns
x.remove(y) # Label dışındaki tüm sütunlar tahmin edici (feature) olacak

from h2o.automl import H2OAutoML

# 1. AutoML Ayarlarını Yapalım
# max_models=10: En iyi 10 modeli bulana kadar dene
# include_algos: Özellikle istediğiniz klasik modelleri dahil ediyoruz
# seed=42: Sonuçların tekrarlanabilir olması için
aml = H2OAutoML(
    max_models=15,
    include_algos=["DRF", "GBM", "GLM"],
    seed=42,
    max_runtime_secs=900 # 10 dakika sınırı (verinizin boyutuna göre artırabilirsiniz)
)

# 2. Eğitimi Başlatalım
# x: Bağımsız değişkenler (TF-IDF sütunları)
# y: Hedef değişken (Label)
# training_frame: Eğitim verisi
# validation_frame: Eğitim sırasında kontrol edilecek doğrulama verisi
aml.train(x=x, y=y, training_frame=train, validation_frame=valid)

# 3. Liderlik Tablosunu Görüntüleyelim
lb = aml.leaderboard
print("Model Liderlik Tablosu (Leaderboard):")
print(lb.head(rows=lb.nrows)) # Tüm modelleri ve başarı metriklerini (AUC, logloss vb.) gösterir

# 1. En iyi modeli (Leader) alalım
best_model = aml.leader

# 2. Test seti üzerinde tahmin yapalım
predictions = best_model.predict(test)

# 3. Modelin genel performans metriklerini görelim
performance = best_model.model_performance(test)
print(performance)

# Test verisi üzerindeki Karmaşıklık Matrisi
print(performance.confusion_matrix())

# En önemli 20 kelimeyi/özelliği görselleştirelim
best_model.varimp_plot(num_of_features=20)

def predict_new_article(new_text):
    # 1. Metin Ön İşleme (Adım 2 ve 3'teki işlemlerin aynısı)
    cleaned = clean_text(new_text) # Regex temizliği
    processed = preprocess_nlp(cleaned) # Stopwords ve Lemmatization

    # 2. Vektörleştirme (Eğitilmiş TF-IDF modelini kullanıyoruz)
    # Not: fit_transform değil, sadece transform kullanmalısın!
    vectorized = tfidf.transform([processed])

    # 3. H2O Frame'e dönüştürme
    vectorized_df = pd.DataFrame(vectorized.toarray(), columns=tfidf.get_feature_names_out())
    h2o_input = h2o.H2OFrame(vectorized_df)

    # 4. Tahmin Yapma
    prediction = best_model.predict(h2o_input)

    # Sonuçları alalım
    res = prediction.as_data_frame()
    label = "HUMAN" if res['predict'][0] == 1 else "AI"
    probability = res.iloc[0, 1:].max() # En yüksek olasılık değeri

    return label, probability

# --- TEST ETME ---
input_article = input("Bir Makale Giriniz: ")

label, confidence = predict_new_article(input_article)

print(f"Tahmin: {label}")
print(f"Güven Skoru: %{confidence * 100:.2f}")

import os

# Modellerin kaydedileceği bir klasör oluşturalım
model_path = "./saved_models"
if not os.path.exists(model_path):
    os.makedirs(model_path)

# 1. En iyi modeli (AutoML Leader) kaydet
# model_path: Kaydedilecek klasör, force=True: Varsa üzerine yaz
model_file = h2o.save_model(model=best_model, path=model_path, force=True)

print(f"En iyi model şuraya kaydedildi: {model_file}")

import joblib

# TF-IDF modelini kaydet (Bu çok önemli!)
joblib.dump(tfidf, './saved_models/tfidf_vectorizer.pkl')
print("TF-IDF vektörleştirici kaydedildi.")

import h2o
import os
import joblib

# 1. Kayıt klasörünü hazırla
save_dir = "./h2o_top_3_models"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# 2. Liderlik tablosundaki ilk 3 modelin ID'sini al
# aml, eğittiğin H2OAutoML nesnesidir
lb = aml.leaderboard
top_3_ids = lb.head(3).as_data_frame()['model_id'].tolist()

print(f"Kaydedilecek modeller: {top_3_ids}")

# 3. Modelleri döngü ile kaydet
for model_id in top_3_ids:
    # Model objesini ID ile çağır
    model_obj = h2o.get_model(model_id)

    # Modeli belirtilen klasöre binary olarak kaydet
    # force=True dosyalar varsa üzerine yazar
    model_path = h2o.save_model(model=model_obj, path=save_dir, force=True)
    print(f"Başarıyla kaydedildi: {model_path}")

# 4. TF-IDF Vektörleştiriciyi de yanına ekle (VS Code tahmini için ŞART)
# tfidf, Adım 4'te oluşturduğun nesnedir
joblib.dump(tfidf, os.path.join(save_dir, 'tfidf_vectorizer.pkl'))
print(f"\nÖNEMLİ: TF-IDF vektörleştirici de '{save_dir}' klasörüne kaydedildi.")

from transformers import AutoTokenizer

# RoBERTa için tokenizer yükleme
tokenizer = AutoTokenizer.from_pretrained("roberta-base")

def tokenize_function(examples):
    return tokenizer(examples["Text"], padding="max_length", truncation=True)

from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments

# Modeli yükle (2 sınıf için: AI ve İnsan)
model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=2)

!pip install evaluate
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import torch
import numpy as np
import evaluate

# Bilgisayarınızda GPU (Ekran Kartı) varsa onu kullanacağız, yoksa CPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Kullanılan cihaz: {device}")

# 'Processed_Text' (lemmatize edilmiş temiz metin) ve 'Label' sütunlarını kullanıyoruz
# Hugging Face 'text' ve 'label' isimlerini standart olarak bekler
hf_data = Dataset.from_pandas(df[['Processed_Text', 'Label']])
hf_data = hf_data.rename_column("Processed_Text", "text")
hf_data = hf_data.rename_column("Label", "label")

# Veriyi %80 Eğitim, %20 Test olarak bölelim
dataset_split = hf_data.train_test_split(test_size=0.2)
train_dataset = dataset_split['train']
test_dataset = dataset_split['test']

# BERT için hazırlık
bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# RoBERTa için hazırlık
roberta_tokenizer = AutoTokenizer.from_pretrained("roberta-base")

def tokenize_function(examples, tokenizer):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)

# Tokenize işlemlerini uygulayalım
tokenized_train_bert = train_dataset.map(lambda x: tokenize_function(x, bert_tokenizer), batched=True)
tokenized_test_bert = test_dataset.map(lambda x: tokenize_function(x, bert_tokenizer), batched=True)

tokenized_train_roberta = train_dataset.map(lambda x: tokenize_function(x, roberta_tokenizer), batched=True)
tokenized_test_roberta = test_dataset.map(lambda x: tokenize_function(x, roberta_tokenizer), batched=True)

# Başarı metriklerini hesaplayan fonksiyon (Accuracy ve F1)
metric = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Eğitim Ayarları
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5, # Transformerlar için ideal öğrenme oranı
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3, # Veri setin küçük olduğu için 3 epoch yeterli olabilir
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
)

# 1. BERT MODELİNİ EĞİTELİM
bert_model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2).to(device)

bert_trainer = Trainer(
    model=bert_model,
    args=training_args,
    train_dataset=tokenized_train_bert,
    eval_dataset=tokenized_test_bert,
    compute_metrics=compute_metrics,
)

print("BERT Eğitimi Başlıyor...")
bert_trainer.train()

# 2. RoBERTa MODELİNİ EĞİTELİM
roberta_model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=2).to(device)

roberta_trainer = Trainer(
    model=roberta_model,
    args=training_args,
    train_dataset=tokenized_train_roberta,
    eval_dataset=tokenized_test_roberta,
    compute_metrics=compute_metrics,
)

print("RoBERTa Eğitimi Başlıyor...")
roberta_trainer.train()

# BERT Sonuçları
bert_eval = bert_trainer.evaluate()
print("--- BERT Test Sonuçları ---")
print(bert_eval)

# RoBERTa Sonuçları
roberta_eval = roberta_trainer.evaluate()
print("\n--- RoBERTa Test Sonuçları ---")
print(roberta_eval)

import torch.nn.functional as F

def predict_with_transformer(text, model, tokenizer):
    # 1. Aynı ön işlemleri uygula (Temizlik + Lemmatization)
    cleaned = clean_text(text)
    processed = preprocess_nlp(cleaned)

    # 2. Tokenization
    inputs = tokenizer(processed, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)

    # 3. Tahmin (Modelin Gradyan hesaplamasına gerek yok)
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        # Ham çıktıları (logits) olasılığa çevir (Softmax)
        probabilities = F.softmax(outputs.logits, dim=-1)

    # 4. Sonuçları al
    probs = probabilities.cpu().numpy()[0]
    prediction_idx = np.argmax(probs)

    # Etiket eşleştirme (Label 0: AI, Label 1: Human varsayımıyla)
    label = "HUMAN" if prediction_idx == 1 else "AI"
    confidence = probs[prediction_idx]

    return label, confidence

# --- DENEME YAPALIM ---
yeni_makale = input("Makale Giriniz: ")

# BERT ile Dene
label_bert, score_bert = predict_with_transformer(yeni_makale, bert_model, bert_tokenizer)
print(f"BERT Tahmini: {label_bert} (Güven: %{score_bert*100:.2f})")

# RoBERTa ile Dene
label_roberta, score_roberta = predict_with_transformer(yeni_makale, roberta_model, roberta_tokenizer)
print(f"RoBERTa Tahmini: {label_roberta} (Güven: %{score_roberta*100:.2f})")

import os

# Kayıt klasörlerini oluşturalım
if not os.path.exists("./saved_transformer_models"):
    os.makedirs("./saved_transformer_models/bert_model")
    os.makedirs("./saved_transformer_models/roberta_model")

# 1. BERT Modeli ve Tokenizer'ını Kaydet
bert_model.save_pretrained("./saved_transformer_models/bert_model")
bert_tokenizer.save_pretrained("./saved_transformer_models/bert_model")
print("BERT modeli ve sözlüğü başarıyla kaydedildi.")

# 2. RoBERTa Modeli ve Tokenizer'ını Kaydet
roberta_model.save_pretrained("./saved_transformer_models/roberta_model")
roberta_tokenizer.save_pretrained("./saved_transformer_models/roberta_model")
print("RoBERTa modeli ve sözlüğü başarıyla kaydedildi.")

import os
import shutil
from google.colab import files

# 1. Klasörü zip dosyasına dönüştür
# 'saved_transformer_models' -> Kaynak klasör adı
# 'transformer_models_archive' -> Oluşturulacak zip dosyasının adı
shutil.make_archive('transformer_models_archive', 'zip', './saved_transformer_models')

# 2. Oluşturulan zip dosyasını indir
files.download('transformer_models_archive.zip')